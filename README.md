## AWSを活用したデータパイプライン構築プロジェクト

### 概要

このプロジェクトは、AWSの各サービスを連携させた**求人情報データパイプライン**です。スクレイピングからデータ処理、データベースへの格納、そしてダッシュボードによる可視化までの一連の流れを自動化しています。

この成果物は、定型的なデータ収集・加工プロセスを自動化し、ビジネスにおけるデータフローの理解やパイプライン構築することを目的としています。

-----

### アーキテクチャ

1.  **データ収集**: AWS Lambda関数がAPI経由でデータ職種の求人情報を取得し、生データ（raw data）を**S3**バケットに保存します。
2.  **データ処理**: EC2インスタンス上で動作する`cron`ジョブが、S3から生データを取得し、Pythonスクリプトで必要な加工を行います。
3.  **データ格納**: 加工後のデータは、再びS3バケットに格納されるとともに、**RDS for PostgreSQL**データベースに書き込まれます。
4.  **データ可視化**: 別のEC2インスタンスでホストされた**Streamlit**アプリケーションが、RDSから最新のデータを取得し、動的なダッシュボードとして可視化します。

-----

### 技術スタック

  * **データ収集**: AWS Lambda, Python
  * **データレイク**: Amazon S3
  * **データ処理・スケジューリング**: Amazon EC2, `cron`, Python
  * **データベース**: Amazon RDS for PostgreSQL
  * **可視化**: Streamlit

-----

### セットアップと使い方

#### 1\. データ処理環境の構築 (EC2)

プロジェクトのリポジトリをEC2インスタンスにクローンし、Pythonの仮想環境と必要なライブラリをインストールします。

#### 2\. `crontab`の設定

EC2上で以下のコマンドを実行し、データ処理スクリプトを自動化します。権限の問題を回避するため、ログは専用ディレクトリに保存します。

```bash
*/30 * * * * /bin/bash -c 'source /home/ec2-user/job_pipeline/.venv/bin/activate && /home/ec2-user/job_pipeline/.venv/bin/python3 /home/ec2-user/job_pipeline/src/transform.py' >> /var/log/job_pipeline/cron_output.log 2>&1
```

-----

### トラブルシューティング

このプロジェクトの構築過程で、いくつかの技術的な課題に直面し、解決しました。

  * **`crond`の挙動**: `crontab`の最終行に改行がなかった、または`cron`サービス名が`crond`だった。
  * **権限エラー**: `crond`が`/var/log`に直接ログを書き込む権限がなかったため、`/var/log`内に専用のディレクトリを作成し、`sudo chown`で所有権を付与することで問題を解決しました。
  * **外部サービスへの接続**: EC2からRDSへの接続には、セキュリティグループでEC2のIPアドレスまたはセキュリティグループIDからのインバウンドルールを許可する必要があることを確認しました。

### 今後の展望

  * **エラー通知**: Lambda関数やEC2のcronジョブでエラーが発生した場合、SlackやEmailで自動的に通知する機能を実装し、運用の信頼性を高めます。
  * **スケーラビリティ**: より大規模なデータに対応するため、EC2のインスタンスサイズを動的に変更するオートスケーリングを検討します。