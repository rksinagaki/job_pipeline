## AWSを活用したデータパイプライン構築プロジェクト

### 概要

このプロジェクトは、AWSの各サービスを連携させた**求人情報データパイプライン**です。スクレイピングからデータ処理、データベースへの格納、そしてダッシュボードによる可視化までの一連の流れを自動化しています。

この成果物は、定型的なデータ収集・加工プロセスを自動化し、ビジネスにおけるデータ活用の基盤を構築することを目的としています。

-----

### 目的

本プロジェクトは、以下の複数の目的を達成するために構築されました。

  * **キャリア市場の分析：** データ職種の求人情報を可視化することで、自身のスキルと市場でのギャップを客観的に把握し、自身のキャリアについて考察します。
  * **技術トレンドの把握：** 求人情報から、現在需要が高い技術やツール、業界の動向を分析し、今後のキャリアプランや学習計画に役立てます。
  * **データフローの理解深化：** 小規模なデータパイプラインを一から構築することで、データの流れ（収集、加工、保存）と各工程の役割に関する実践的な理解を深めます。
  * **クラウドプラットフォームの習得：** 複数のAWSサービス（Lambda, S3, EC2, RDS）を組み合わせることで、クラウド上でのインフラ構築とサービス連携に関する知識と経験を習得します。

-----

### アーキテクチャ

1.  **データ収集：** AWS Lambda関数がAPI経由でデータ職種の求人情報を取得し、生データ（raw data）を**S3**バケットに保存します。
2.  **データ処理：** EC2インスタンス上で動作する`cron`ジョブが、S3から生データを取得し、Pythonスクリプトで必要な加工を行います。
3.  **データ格納：** 加工後のデータは、再びS3バケットに格納されるとともに、**RDS for PostgreSQL**データベースに書き込まれます。
4.  **データ可視化：** 別のEC2インスタンスでホストされた**Streamlit**アプリケーションが最新のデータを取得し、動的なダッシュボードとして可視化します。

-----

### 技術スタック

  * **データ収集：** AWS Lambda, Python
  * **データレイク：** Amazon S3
  * **データ処理・スケジューリング：** Amazon EC2, `cron`, Python
  * **データベース：** Amazon RDS for PostgreSQL
  * **可視化：** Streamlit

-----

### 主要ライブラリ

* `boto3`
* `s3fs`
* `pandas`
* `numpy`
* `streamlit`
* `psycopg2-binary`
* `sqlalchemy`
* `plotly`
* `python-dotenv`

-----

### トラブルシューティング

このプロジェクトの構築過程で、いくつかの技術的な課題に直面し、解決しました。

  * **`crond`の挙動：** `crontab`の最終行に改行がなかった、または`cron`サービス名が`crond`だった。
  
  * **外部サービスへの接続：** EC2からRDSへの接続には、セキュリティグループでEC2のIPアドレスまたはセキュリティグループIDからのインバウンドルールを許可する必要があることを確認しました。

  * **Lambdaでのライブラリ管理：** `requests`などの外部ライブラリをLambdaにデプロイする際、依存関係のライブラリも`--target`オプションで含める必要があることを特定。また、Pandasのような容量の大きなライブラリは、Lambdaのデプロイパッケージの推奨サイズを超えるため、Lambdaレイヤーを使用するか、EC2上での処理に切り替える必要があることを学びました。

---

### 今後の展望

本プロジェクトの経験を基に、以下のようなステップでさらなる技術的深化を目指します。

* **パイプラインの拡張とスケーリング：**
  現状の小規模なパイプラインから、より大規模なデータに対応できるアーキテクチャへの移行を目指します。具体的には、AWS Glueを活用したETL（抽出、変換、ロード）処理の構築や、Redshiftのようなスケーラブルなデータウェアハウスを組み込んだパイプラインの実装を検討します。

* **リアルタイム・イベント駆動型アーキテクチャへの移行：**<br>
  現在の`cron`や`AWS EventScheduler`による定期的なスケジューリングから脱却し、より効率的なデータ処理フローを構築します。例えば、Lambda関数によるデータ収集やS3へのデータ格納をトリガーとして、自動的に後続の処理（データ加工、RDSへの格納など）が開始されるような、**イベント駆動型**のパイプライン構築に着手します。